{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Big part about learning DL is quite intuitive!) Notes taken from Lesson 1, 2 fastbook\n",
    "## Deep Learning: Neural Network\n",
    "Isn’t it fascinating that we are trying to understand what our brains are doing in order to figure out how to effectively build a learning model. As explained in Parallel Distributed Processing (PDP) by David Rumelhart, “people are smarter than today’s computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at.” ‘This natural computational framework in our brain is essentially what we also want to deploy in our machines. \n",
    "\n",
    "Arthur Samuel (former IBM researcher) shares a similar sentiment. In his classic 1962 essay “Artificial Intelligence: a Frontier of Automation,” he wrote “programming a computer for such computations is, at best, a difficult task, not primarily because of any inherent complexity in the computer itself but rather, because of the need to spell out every minute step of the process in the snot exasperating detail. Computers, as any programmer will tell you, are giant morons, not giant brains.” His reasoning is simple: instead of writing out every single exasperatingly detailed instructions for your machine, teaching the machine to learn (deploy) a model to solve problems is more efficient in the long run. \n",
    "\n",
    "Deep learning is deep because we don’t exactly know how to compute a universal labeling system for all cats and dogs. Unlike linear regression where we can see each weight (beta-coefficients) linked to each factor, deep neural network has multiple hidden layers where they optimize its parameters to best perform the task given \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of DL models\n",
    "- A model cannot be create without data, which means a model can only learn to operate on the patterns seen in the input data used to train it \n",
    "    - e.g) Recommendation system based on user behavior is likely to provide you with a list of items that the user already has or already knows about (because it is based on their purchase history). Recommendation might be accurate (because it matches user history) but may not be helpful (because the user might want to know something new) — how do we incorporate this 'new-ness' in to machines? If it is only trained by machines? It recommends what a user might like but not what would be helpful to users \n",
    "\n",
    "- More bias in data translates to bias in model\n",
    "- This learning approach only creates predictions and not recommended actions\n",
    "    - Remember from IB, TOK we talked about whether people are natural pattern seekers? Finding patterns might be a good way of learning about things, but it also has its limitations \n",
    "    - What about emotional learning - i guess this is more related to retaining knowledge (from the article \"we learn only to forget\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "- error rate/ accuracy rate: measures the quality of model’s prediction \n",
    "- Loss: measure the performance (so that the training system can use to update weights automatically) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Terminologies\n",
    "- Label - catagorization results \n",
    "- Architecture - template of the model (the actual mathematical functions vlbs)\n",
    "- Model - architecture + parameters\n",
    "- Fit (or train) - update parameters so that the predictions of the model using the input data matches the target labels\n",
    "- Pretrained model: model that is already trained using a larger dataset (and will be fine-tuned)\n",
    "- Fine-tune - update a pretrained model for a different task \n",
    "- Epoch - one complete pass through the input data\n",
    "- Loss - measure of model performance\n",
    "- Metric - measure of model accuracy\n",
    "- Validation set/ training set/ \n",
    "- CNN: convolutional neural network (a type of neural network that works particularly well for computer vision tasks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "- goal: validation and test sets must be representative of the new data you will see in the future. Test set is for final evaluation. Validation set is a part of human conditioning hyperparameters process and thus a part of the ‘training’ model process and so it will inevitably increase the risk of overfitting. Overfitting happens because of a limited set of data and it does not generalize well to unseen data \n",
    "\n",
    "### GPU\n",
    "- graphics processing unit is specialized for displaying graphics. The hardware optimization used in GPUs allow it to handle thousands of tasks at the same time. /incidentally, these optimizations allow us to run and train neural networks hundreds of times faster than a regular CPU\n",
    "\n",
    "### Process of building and training model\n",
    "- 1) architecture (mathematical functions) of your problem\n",
    "- 2) collect and clean data to train ur model. Remember to have validation, test, and training set. Test set is something u have and will never touch. Validation set is to check how it’s doing. Training set builds the model. Label data correctly.\n",
    "- 3) define loss function to quantitatively measure the performance and we need a way to update these parameters in order to improve its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: DataLoaders\n",
    "We have to specify these four things: \n",
    "- what kinds of data we are working with\n",
    "- how to get the list of items\n",
    "- how to label these items\n",
    "- how to create the validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4757cde43cbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dls = DataBlock(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mblocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImageBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoryBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mget_items\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_image_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msplitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRandomSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_pct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mget_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataBlock' is not defined"
     ]
    }
   ],
   "source": [
    "dls = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock), \n",
    "    get_items=get_image_files, \n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    get_y=parent_label,\n",
    "    item_tfms=[Resize(192, method='squish')]\n",
    ").dataloaders(path)\n",
    "\n",
    "dls.show_batch(max_n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "- it is used to generalize our limited set of training data. Examples include flipping, rotation, perspective warping, brightness changes, etc. Data augmentation is useful for the model to better understand the basic concept of what an object is and how the objects of interest are represented in images. Therefore, data augmentation allows machine learning models to generalize . This is especially important when it can be slow and expensive to label data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "- it is a representation of the predictions made vs the correct labels. The rows of the matrix represent the actual labels while the columns represent the predictions. Therefore, the number of images in the diagonal elements represent the number of correctly classified images, while the off-diagonal elements are incorrectly classified images. Class confusion matrices provide useful information about how well the model is doing and which classes the model might be confusing .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export() - Learner\n",
    "export saves both the architecture, as well as the trained parameters of the neural network architecture. It also saves how the DataLoaders are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e735dc063432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# export learner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_exts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#saved in pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "# export learner\n",
    "learn.export()\n",
    "path = Path()\n",
    "path.ls(file_exts='.pkl') #saved in pickle\n",
    "\n",
    "# load learner\n",
    "learn_inf = load_learner(path/'export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
